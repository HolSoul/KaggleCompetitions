{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n",
      "Submission file 'baseline_submission.csv' created successfully!\n",
      "  PassengerId  Transported\n",
      "0     0013_01         True\n",
      "1     0018_01        False\n",
      "2     0019_01         True\n",
      "3     0021_01         True\n",
      "4     0023_01         True\n",
      "Training Logistic Regression...\n",
      "\n",
      "--- Blending Predictions ---\n",
      "\n",
      "Submission file 'ensemble_submission.csv' created successfully!\n",
      "  PassengerId  Transported\n",
      "0     0013_01         True\n",
      "1     0018_01        False\n",
      "2     0019_01         True\n",
      "3     0021_01         True\n",
      "4     0023_01         True\n",
      "--- Fold 2/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1908\n",
      "[LightGBM] [Info] Number of data points in the train set: 7823, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503643 -> initscore=0.014573\n",
      "[LightGBM] [Info] Start training from score 0.014573\n",
      "--- Fold 3/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7823, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503643 -> initscore=0.014573\n",
      "[LightGBM] [Info] Start training from score 0.014573\n",
      "--- Fold 4/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3941, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1908\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503707 -> initscore=0.014826\n",
      "[LightGBM] [Info] Start training from score 0.014826\n",
      "--- Fold 5/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3941, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1908\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503707 -> initscore=0.014826\n",
      "[LightGBM] [Info] Start training from score 0.014826\n",
      "--- Fold 6/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "--- Fold 7/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "--- Fold 8/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "--- Fold 9/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1908\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "--- Fold 10/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "\n",
      "--- Training Meta-Model ---\n",
      "\n",
      "Submission file 'stacking_submission.csv' created successfully!\n",
      "  PassengerId  Transported\n",
      "0     0013_01         True\n",
      "1     0018_01        False\n",
      "2     0019_01         True\n",
      "3     0021_01         True\n",
      "4     0023_01         True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "sample_submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "\n",
    "test_passenger_ids = test_df['PassengerId']\n",
    "train_ids = train_df['PassengerId']\n",
    "\n",
    "y = train_df['Transported'].copy()\n",
    "train_df.drop('Transported', axis=1, inplace=True)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "all_data['GroupId'] = all_data['PassengerId'].apply(lambda x: x.split('_')[0])\n",
    "all_data['GroupSize'] = all_data.groupby('GroupId')['PassengerId'].transform('count')\n",
    "\n",
    "all_data[['Deck', 'CabinNum', 'Side']] = all_data['Cabin'].str.split('/', expand=True)\n",
    "all_data['CabinNum'] = pd.to_numeric(all_data['CabinNum'])\n",
    "\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "all_data['NoSpend'] = (all_data['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "for col in spend_cols:\n",
    "    all_data.loc[(all_data['CryoSleep'] == True) & (all_data[col].isnull()), col] = 0\n",
    "\n",
    "numeric_cols = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = all_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    median_val = all_data[col].median()\n",
    "    all_data[col] = all_data[col].fillna(median_val)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    mode_val = all_data[col].mode()[0]\n",
    "    all_data[col] = all_data[col].fillna(mode_val)\n",
    "\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "\n",
    "all_data.drop(['PassengerId', 'Name', 'Cabin', 'GroupId'], axis=1, inplace=True)\n",
    "\n",
    "bool_cols = ['VIP', 'CryoSleep']\n",
    "for col in bool_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].astype(int)\n",
    "\n",
    "categorical_cols_to_encode = all_data.select_dtypes(include=['object']).columns\n",
    "all_data = pd.get_dummies(all_data, columns=categorical_cols_to_encode, dummy_na=False)\n",
    "\n",
    "X = all_data.iloc[:len(train_ids)]\n",
    "X_test = all_data.iloc[len(train_ids):]\n",
    "y = y.astype(int)\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(random_state=42, n_estimators=200, learning_rate=0.05, num_leaves=20)\n",
    "lgbm.fit(X, y)\n",
    "predictions = lgbm.predict(X_test)\n",
    "\n",
    "submission_preds = (predictions == 1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Transported': submission_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('../submissions/baseline_submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'baseline_submission.csv' created successfully!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Ensemble Models ---\n",
      "Training LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n",
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (Our proven pipeline)\n",
    "# =============================================================================\n",
    "\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "test_passenger_ids = test_df['PassengerId']\n",
    "train_ids = train_df['PassengerId']\n",
    "y = train_df['Transported'].copy()\n",
    "\n",
    "# Drop target for concatenation\n",
    "train_df.drop('Transported', axis=1, inplace=True)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "all_data['GroupId'] = all_data['PassengerId'].apply(lambda x: x.split('_')[0])\n",
    "all_data['GroupSize'] = all_data.groupby('GroupId')['PassengerId'].transform('count')\n",
    "all_data[['Deck', 'CabinNum', 'Side']] = all_data['Cabin'].str.split('/', expand=True)\n",
    "all_data['CabinNum'] = pd.to_numeric(all_data['CabinNum'])\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "all_data['NoSpend'] = (all_data['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "# --- Imputing Missing Values ---\n",
    "for col in spend_cols:\n",
    "    all_data.loc[(all_data['CryoSleep'] == True) & (all_data[col].isnull()), col] = 0\n",
    "\n",
    "numeric_cols = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = all_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].median())\n",
    "for col in categorical_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "\n",
    "# --- Final Prep ---\n",
    "all_data.drop(['PassengerId', 'Name', 'Cabin', 'GroupId'], axis=1, inplace=True)\n",
    "bool_cols = ['VIP', 'CryoSleep']\n",
    "for col in bool_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].astype(int)\n",
    "categorical_cols_to_encode = all_data.select_dtypes(include=['object']).columns\n",
    "all_data = pd.get_dummies(all_data, columns=categorical_cols_to_encode, dummy_na=False)\n",
    "\n",
    "# Separate train and test\n",
    "X = all_data.iloc[:len(train_ids)]\n",
    "X_test = all_data.iloc[len(train_ids):]\n",
    "y = y.astype(int)\n",
    "\n",
    "# Scale data for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Model Training\n",
    "# =============================================================================\n",
    "print(\"--- Training Ensemble Models ---\")\n",
    "\n",
    "# Define models\n",
    "lgbm = lgb.LGBMClassifier(random_state=42, n_estimators=200, learning_rate=0.05, num_leaves=20)\n",
    "xgboost = xgb.XGBClassifier(random_state=42, n_estimators=200, learning_rate=0.05, max_depth=5, use_label_encoder=False, eval_metric='logloss')\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Train models\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm.fit(X, y)\n",
    "print(\"Training XGBoost...\")\n",
    "xgboost.fit(X, y)\n",
    "print(\"Training Logistic Regression...\")\n",
    "logreg.fit(X_scaled, y) # Use scaled data for logistic regression\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Blending Predictions and Creating Submission\n",
    "# =============================================================================\n",
    "print(\"\\n--- Blending Predictions ---\")\n",
    "\n",
    "# Predict probabilities for the positive class (Transported=True)\n",
    "lgbm_probs = lgbm.predict_proba(X_test)[:, 1]\n",
    "xgboost_probs = xgboost.predict_proba(X_test)[:, 1]\n",
    "logreg_probs = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Weighted average of probabilities\n",
    "final_probs = (0.4 * lgbm_probs + 0.4 * xgboost_probs + 0.2 * logreg_probs)\n",
    "\n",
    "# Convert probabilities to final predictions (True if > 0.5)\n",
    "final_preds = (final_probs > 0.5)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({'PassengerId': test_passenger_ids, 'Transported': final_preds})\n",
    "submission.to_csv('../submissions/ensemble_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'ensemble_submission.csv' created successfully!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Stacking ---\n",
      "--- Fold 1/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7823, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503643 -> initscore=0.014573\n",
      "[LightGBM] [Info] Start training from score 0.014573\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (Our proven pipeline)\n",
    "# =============================================================================\n",
    "\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "test_passenger_ids = test_df['PassengerId']\n",
    "train_ids = train_df['PassengerId']\n",
    "y = train_df['Transported'].copy().astype(int)\n",
    "\n",
    "train_df.drop('Transported', axis=1, inplace=True)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "all_data['GroupId'] = all_data['PassengerId'].apply(lambda x: x.split('_')[0])\n",
    "all_data['GroupSize'] = all_data.groupby('GroupId')['PassengerId'].transform('count')\n",
    "all_data[['Deck', 'CabinNum', 'Side']] = all_data['Cabin'].str.split('/', expand=True)\n",
    "all_data['CabinNum'] = pd.to_numeric(all_data['CabinNum'])\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "all_data['NoSpend'] = (all_data['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "for col in spend_cols:\n",
    "    all_data.loc[(all_data['CryoSleep'] == True) & (all_data[col].isnull()), col] = 0\n",
    "\n",
    "numeric_cols = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = all_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].median())\n",
    "for col in categorical_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "\n",
    "all_data.drop(['PassengerId', 'Name', 'Cabin', 'GroupId'], axis=1, inplace=True)\n",
    "bool_cols = ['VIP', 'CryoSleep']\n",
    "for col in bool_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].astype(int)\n",
    "categorical_cols_to_encode = all_data.select_dtypes(include=['object']).columns\n",
    "all_data = pd.get_dummies(all_data, columns=categorical_cols_to_encode, dummy_na=False)\n",
    "\n",
    "X = all_data.iloc[:len(train_ids)]\n",
    "X_test = all_data.iloc[len(train_ids):]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Stacking Implementation\n",
    "# =============================================================================\n",
    "print(\"--- Starting Stacking ---\")\n",
    "\n",
    "N_SPLITS = 10\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store out-of-fold predictions\n",
    "oof_preds_lgbm = np.zeros((len(X),))\n",
    "oof_preds_xgb = np.zeros((len(X),))\n",
    "oof_preds_logreg = np.zeros((len(X),))\n",
    "\n",
    "# Create empty arrays to store test predictions\n",
    "test_preds_lgbm = np.zeros((len(X_test),))\n",
    "test_preds_xgb = np.zeros((len(X_test),))\n",
    "test_preds_logreg = np.zeros((len(X_test),))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Scaled data for logistic regression\n",
    "    X_train_scaled, X_val_scaled = X_scaled[train_idx], X_scaled[val_idx]\n",
    "\n",
    "    # --- Train and predict with LGBM ---\n",
    "    lgbm = lgb.LGBMClassifier(random_state=42, n_estimators=200, learning_rate=0.05, num_leaves=20)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    oof_preds_lgbm[val_idx] = lgbm.predict_proba(X_val)[:, 1]\n",
    "    test_preds_lgbm += lgbm.predict_proba(X_test)[:, 1] / N_SPLITS\n",
    "\n",
    "    # --- Train and predict with XGBoost ---\n",
    "    xgboost = xgb.XGBClassifier(random_state=42, n_estimators=200, learning_rate=0.05, max_depth=5, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    oof_preds_xgb[val_idx] = xgboost.predict_proba(X_val)[:, 1]\n",
    "    test_preds_xgb += xgboost.predict_proba(X_test)[:, 1] / N_SPLITS\n",
    "\n",
    "    # --- Train and predict with Logistic Regression ---\n",
    "    logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg.fit(X_train_scaled, y_train)\n",
    "    oof_preds_logreg[val_idx] = logreg.predict_proba(X_val_scaled)[:, 1]\n",
    "    test_preds_logreg += logreg.predict_proba(X_test_scaled)[:, 1] / N_SPLITS\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Train Meta-Model and Create Submission\n",
    "# =============================================================================\n",
    "print(\"\\n--- Training Meta-Model ---\")\n",
    "\n",
    "# Create the training data for the meta-model\n",
    "meta_X = np.stack([oof_preds_lgbm, oof_preds_xgb, oof_preds_logreg], axis=1)\n",
    "\n",
    "# Create the test data for the meta-model\n",
    "meta_X_test = np.stack([test_preds_lgbm, test_preds_xgb, test_preds_logreg], axis=1)\n",
    "\n",
    "# Meta-model (Logistic Regression is a good choice)\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(meta_X, y)\n",
    "\n",
    "# Make final predictions\n",
    "final_probs = meta_model.predict_proba(meta_X_test)[:, 1]\n",
    "final_preds = (final_probs > 0.5)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({'PassengerId': test_passenger_ids, 'Transported': final_preds})\n",
    "submission.to_csv('../submissions/stacking_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'stacking_submission.csv' created successfully!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (Our proven pipeline)\n",
    "# =============================================================================\n",
    "\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "test_passenger_ids = test_df['PassengerId']\n",
    "train_ids = train_df['PassengerId']\n",
    "y = train_df['Transported'].copy().astype(int)\n",
    "\n",
    "train_df.drop('Transported', axis=1, inplace=True)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "all_data['GroupId'] = all_data['PassengerId'].apply(lambda x: x.split('_')[0])\n",
    "all_data['GroupSize'] = all_data.groupby('GroupId')['PassengerId'].transform('count')\n",
    "all_data[['Deck', 'CabinNum', 'Side']] = all_data['Cabin'].str.split('/', expand=True)\n",
    "all_data['CabinNum'] = pd.to_numeric(all_data['CabinNum'])\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "all_data['NoSpend'] = (all_data['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "for col in spend_cols:\n",
    "    all_data.loc[(all_data['CryoSleep'] == True) & (all_data[col].isnull()), col] = 0\n",
    "\n",
    "numeric_cols = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = all_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].median())\n",
    "for col in categorical_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "\n",
    "all_data.drop(['PassengerId', 'Name', 'Cabin', 'GroupId'], axis=1, inplace=True)\n",
    "bool_cols = ['VIP', 'CryoSleep']\n",
    "for col in bool_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].astype(int)\n",
    "categorical_cols_to_encode = all_data.select_dtypes(include=['object']).columns\n",
    "all_data = pd.get_dummies(all_data, columns=categorical_cols_to_encode, dummy_na=False)\n",
    "\n",
    "X = all_data.iloc[:len(train_ids)]\n",
    "X_test = all_data.iloc[len(train_ids):]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Hyperparameter Tuning with Optuna for LightGBM\n",
    "# =============================================================================\n",
    "print(\"--- Starting Hyperparameter Tuning for LightGBM ---\")\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, preds))\n",
    "        \n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50) # 50 trials is a good balance\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "best_lgbm_params = trial.params\n",
    "best_lgbm_params['random_state'] = 42\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Final Model Training and Submission\n",
    "# =============================================================================\n",
    "print(\"\\n--- Training Final Ensemble with Tuned LGBM ---\")\n",
    "\n",
    "lgbm_tuned = lgb.LGBMClassifier(**best_lgbm_params)\n",
    "xgboost = xgb.XGBClassifier(random_state=42, n_estimators=200, learning_rate=0.05, max_depth=5, use_label_encoder=False, eval_metric='logloss')\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "print(\"Training Tuned LightGBM...\")\n",
    "lgbm_tuned.fit(X, y)\n",
    "print(\"Training XGBoost...\")\n",
    "xgboost.fit(X, y)\n",
    "print(\"Training Logistic Regression...\")\n",
    "logreg.fit(X_scaled, y)\n",
    "\n",
    "lgbm_probs = lgbm_tuned.predict_proba(X_test)[:, 1]\n",
    "xgboost_probs = xgboost.predict_proba(X_test)[:, 1]\n",
    "logreg_probs = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "final_probs = (0.4 * lgbm_probs + 0.4 * xgboost_probs + 0.2 * logreg_probs)\n",
    "final_preds = (final_probs > 0.5)\n",
    "\n",
    "submission = pd.DataFrame({'PassengerId': test_passenger_ids, 'Transported': final_preds})\n",
    "submission.to_csv('../submissions/tuned_ensemble_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'tuned_ensemble_submission.csv' created successfully!\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
