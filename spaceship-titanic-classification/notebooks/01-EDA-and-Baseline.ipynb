{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n",
      "Submission file 'baseline_submission.csv' created successfully!\n",
      "  PassengerId  Transported\n",
      "0     0013_01         True\n",
      "1     0018_01        False\n",
      "2     0019_01         True\n",
      "3     0021_01         True\n",
      "4     0023_01         True\n",
      "Training Logistic Regression...\n",
      "\n",
      "--- Blending Predictions ---\n",
      "\n",
      "Submission file 'ensemble_submission.csv' created successfully!\n",
      "  PassengerId  Transported\n",
      "0     0013_01         True\n",
      "1     0018_01        False\n",
      "2     0019_01         True\n",
      "3     0021_01         True\n",
      "4     0023_01         True\n",
      "--- Fold 2/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1908\n",
      "[LightGBM] [Info] Number of data points in the train set: 7823, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503643 -> initscore=0.014573\n",
      "[LightGBM] [Info] Start training from score 0.014573\n",
      "--- Fold 3/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7823, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503643 -> initscore=0.014573\n",
      "[LightGBM] [Info] Start training from score 0.014573\n",
      "--- Fold 4/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3941, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1908\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503707 -> initscore=0.014826\n",
      "[LightGBM] [Info] Start training from score 0.014826\n",
      "--- Fold 5/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3941, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1908\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503707 -> initscore=0.014826\n",
      "[LightGBM] [Info] Start training from score 0.014826\n",
      "--- Fold 6/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "--- Fold 7/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "--- Fold 8/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "--- Fold 9/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1908\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "--- Fold 10/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n",
      "[LightGBM] [Info] Start training from score 0.014315\n",
      "\n",
      "--- Training Meta-Model ---\n",
      "\n",
      "Submission file 'stacking_submission.csv' created successfully!\n",
      "  PassengerId  Transported\n",
      "0     0013_01         True\n",
      "1     0018_01        False\n",
      "2     0019_01         True\n",
      "3     0021_01         True\n",
      "4     0023_01         True\n",
      "Training Logistic Regression...\n",
      "\n",
      "Submission file 'tuned_ensemble_submission.csv' created successfully!\n",
      "  PassengerId  Transported\n",
      "0     0013_01         True\n",
      "1     0018_01        False\n",
      "2     0019_01         True\n",
      "3     0021_01         True\n",
      "4     0023_01         True\n",
      "[I 2025-07-21 14:06:40,301] Trial 1 finished with value: 0.7963864925082269 and parameters: {'n_estimators': 303, 'learning_rate': 0.25366905462290484, 'num_leaves': 298, 'max_depth': 10, 'min_child_samples': 58, 'subsample': 0.8843374679738973, 'colsample_bytree': 0.699869496316257}. Best is trial 0 with value: 0.8083508967430324.\n",
      "[I 2025-07-21 14:06:44,481] Trial 2 finished with value: 0.7889098069006499 and parameters: {'n_estimators': 775, 'learning_rate': 0.24000449084158035, 'num_leaves': 123, 'max_depth': 11, 'min_child_samples': 52, 'subsample': 0.7858133875406406, 'colsample_bytree': 0.9438701301806056}. Best is trial 0 with value: 0.8083508967430324.\n",
      "[I 2025-07-21 14:06:45,644] Trial 3 finished with value: 0.8029452928187105 and parameters: {'n_estimators': 608, 'learning_rate': 0.16383496704321954, 'num_leaves': 24, 'max_depth': 5, 'min_child_samples': 73, 'subsample': 0.9385772953328079, 'colsample_bytree': 0.8793171238823219}. Best is trial 0 with value: 0.8083508967430324.\n",
      "[I 2025-07-21 14:06:49,156] Trial 4 finished with value: 0.7919010899350247 and parameters: {'n_estimators': 934, 'learning_rate': 0.20461021512367858, 'num_leaves': 180, 'max_depth': 9, 'min_child_samples': 67, 'subsample': 0.6629817615370192, 'colsample_bytree': 0.8693482149991045}. Best is trial 0 with value: 0.8083508967430324.\n",
      "[I 2025-07-21 14:06:50,502] Trial 5 finished with value: 0.8093874963522149 and parameters: {'n_estimators': 774, 'learning_rate': 0.07977144373167593, 'num_leaves': 61, 'max_depth': 4, 'min_child_samples': 21, 'subsample': 0.8513704095114083, 'colsample_bytree': 0.9023714064051608}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:06:51,231] Trial 6 finished with value: 0.802945491337627 and parameters: {'n_estimators': 402, 'learning_rate': 0.2629309357450887, 'num_leaves': 265, 'max_depth': 4, 'min_child_samples': 54, 'subsample': 0.7270685223710631, 'colsample_bytree': 0.8007886335219073}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:06:57,382] Trial 7 finished with value: 0.7948914465477891 and parameters: {'n_estimators': 956, 'learning_rate': 0.07300832887439672, 'num_leaves': 176, 'max_depth': 12, 'min_child_samples': 33, 'subsample': 0.6150156535502664, 'colsample_bytree': 0.8004978056655897}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:06:58,082] Trial 8 finished with value: 0.801105220981332 and parameters: {'n_estimators': 245, 'learning_rate': 0.23094442849379376, 'num_leaves': 26, 'max_depth': 12, 'min_child_samples': 17, 'subsample': 0.7695354371584802, 'colsample_bytree': 0.7259247015875038}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:06:58,382] Trial 9 finished with value: 0.8078923842188048 and parameters: {'n_estimators': 215, 'learning_rate': 0.2819521640783726, 'num_leaves': 70, 'max_depth': 3, 'min_child_samples': 46, 'subsample': 0.9444050855633235, 'colsample_bytree': 0.6894071325236537}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:00,255] Trial 10 finished with value: 0.8078910607593613 and parameters: {'n_estimators': 651, 'learning_rate': 0.011285628154596578, 'num_leaves': 107, 'max_depth': 6, 'min_child_samples': 100, 'subsample': 0.8476625174656235, 'colsample_bytree': 0.9841488106347785}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:02,468] Trial 11 finished with value: 0.8004131178653129 and parameters: {'n_estimators': 426, 'learning_rate': 0.08776120338963249, 'num_leaves': 216, 'max_depth': 7, 'min_child_samples': 6, 'subsample': 0.9942797480554648, 'colsample_bytree': 0.888123218229997}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:05,674] Trial 12 finished with value: 0.797652977022759 and parameters: {'n_estimators': 756, 'learning_rate': 0.08527330436424561, 'num_leaves': 118, 'max_depth': 8, 'min_child_samples': 26, 'subsample': 0.8576012422407828, 'colsample_bytree': 0.8284293947166611}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:06,331] Trial 13 finished with value: 0.8072007443135911 and parameters: {'n_estimators': 120, 'learning_rate': 0.03278402151730915, 'num_leaves': 229, 'max_depth': 7, 'min_child_samples': 34, 'subsample': 0.9057980439401722, 'colsample_bytree': 0.7500526804181268}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:07,527] Trial 14 finished with value: 0.8027148785295836 and parameters: {'n_estimators': 482, 'learning_rate': 0.12986703638061867, 'num_leaves': 78, 'max_depth': 5, 'min_child_samples': 8, 'subsample': 0.8192457764623257, 'colsample_bytree': 0.9309933042023381}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:11,766] Trial 15 finished with value: 0.7910955001717188 and parameters: {'n_estimators': 800, 'learning_rate': 0.12961228890891568, 'num_leaves': 144, 'max_depth': 9, 'min_child_samples': 21, 'subsample': 0.978981258581392, 'colsample_bytree': 0.6195975523479473}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:12,019] Trial 16 finished with value: 0.7981127468334579 and parameters: {'n_estimators': 105, 'learning_rate': 0.05157063976040867, 'num_leaves': 210, 'max_depth': 3, 'min_child_samples': 38, 'subsample': 0.7332768800552993, 'colsample_bytree': 0.844908022494858}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:13,765] Trial 17 finished with value: 0.7992636271655933 and parameters: {'n_estimators': 565, 'learning_rate': 0.11592364385676307, 'num_leaves': 71, 'max_depth': 6, 'min_child_samples': 16, 'subsample': 0.9058027029026339, 'colsample_bytree': 0.917932172541899}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:15,895] Trial 18 finished with value: 0.7951220593558326 and parameters: {'n_estimators': 688, 'learning_rate': 0.175262813866102, 'num_leaves': 162, 'max_depth': 8, 'min_child_samples': 83, 'subsample': 0.8277282578488603, 'colsample_bytree': 0.9788422467714928}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:17,696] Trial 19 finished with value: 0.8061660637206017 and parameters: {'n_estimators': 831, 'learning_rate': 0.059435470689252756, 'num_leaves': 197, 'max_depth': 5, 'min_child_samples': 40, 'subsample': 0.9448942438523698, 'colsample_bytree': 0.7633083990610254}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:21,789] Trial 20 finished with value: 0.7936265501845895 and parameters: {'n_estimators': 871, 'learning_rate': 0.10238761845399952, 'num_leaves': 91, 'max_depth': 9, 'min_child_samples': 25, 'subsample': 0.750408164607511, 'colsample_bytree': 0.6219230371139158}. Best is trial 5 with value: 0.8093874963522149.\n",
      "[I 2025-07-21 14:07:22,146] Trial 21 finished with value: 0.8106519295046093 and parameters: {'n_estimators': 240, 'learning_rate': 0.28878732755168757, 'num_leaves': 49, 'max_depth': 3, 'min_child_samples': 49, 'subsample': 0.9409378341355006, 'colsample_bytree': 0.6678749076961887}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:22,698] Trial 22 finished with value: 0.8082370792308847 and parameters: {'n_estimators': 303, 'learning_rate': 0.19504847625162247, 'num_leaves': 42, 'max_depth': 4, 'min_child_samples': 12, 'subsample': 0.8792577062894354, 'colsample_bytree': 0.6774377921199449}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:22,983] Trial 23 finished with value: 0.8061659975476296 and parameters: {'n_estimators': 166, 'learning_rate': 0.1382261272429615, 'num_leaves': 54, 'max_depth': 3, 'min_child_samples': 29, 'subsample': 0.919977984228244, 'colsample_bytree': 0.7808350549018472}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:23,605] Trial 24 finished with value: 0.8093870993143819 and parameters: {'n_estimators': 331, 'learning_rate': 0.03790079694667406, 'num_leaves': 141, 'max_depth': 4, 'min_child_samples': 47, 'subsample': 0.97053603744956, 'colsample_bytree': 0.6619006716633774}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:24,299] Trial 25 finished with value: 0.8002987709693876 and parameters: {'n_estimators': 349, 'learning_rate': 0.011102381936676176, 'num_leaves': 134, 'max_depth': 4, 'min_child_samples': 65, 'subsample': 0.9712539387060346, 'colsample_bytree': 0.6522166874596191}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:25,152] Trial 26 finished with value: 0.8078917886620554 and parameters: {'n_estimators': 506, 'learning_rate': 0.0422214253445472, 'num_leaves': 93, 'max_depth': 4, 'min_child_samples': 43, 'subsample': 0.9644287033834354, 'colsample_bytree': 0.6022516797444404}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:26,414] Trial 27 finished with value: 0.8081216735674047 and parameters: {'n_estimators': 392, 'learning_rate': 0.025219294062366417, 'num_leaves': 49, 'max_depth': 6, 'min_child_samples': 51, 'subsample': 0.8484118993079834, 'colsample_bytree': 0.6499814617379818}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:26,981] Trial 28 finished with value: 0.8098470014710252 and parameters: {'n_estimators': 271, 'learning_rate': 0.29076576488113354, 'num_leaves': 146, 'max_depth': 3, 'min_child_samples': 82, 'subsample': 0.9957331377857918, 'colsample_bytree': 0.7222916690774136}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:27,368] Trial 29 finished with value: 0.8055915499761446 and parameters: {'n_estimators': 233, 'learning_rate': 0.2951772415383417, 'num_leaves': 106, 'max_depth': 3, 'min_child_samples': 91, 'subsample': 0.9973941497910831, 'colsample_bytree': 0.7240602181773743}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:28,294] Trial 30 finished with value: 0.8058213025355497 and parameters: {'n_estimators': 708, 'learning_rate': 0.2744912138720712, 'num_leaves': 39, 'max_depth': 3, 'min_child_samples': 80, 'subsample': 0.8766224437988954, 'colsample_bytree': 0.71197526855532}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:28,874] Trial 31 finished with value: 0.8082365498471074 and parameters: {'n_estimators': 294, 'learning_rate': 0.218945177226067, 'num_leaves': 154, 'max_depth': 4, 'min_child_samples': 65, 'subsample': 0.9318360575594987, 'colsample_bytree': 0.6525963975801063}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:29,338] Trial 32 finished with value: 0.8046703560304422 and parameters: {'n_estimators': 169, 'learning_rate': 0.29567810523001326, 'num_leaves': 62, 'max_depth': 5, 'min_child_samples': 59, 'subsample': 0.9566227953216894, 'colsample_bytree': 0.6699356945695722}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:29,978] Trial 33 finished with value: 0.8069709917541861 and parameters: {'n_estimators': 346, 'learning_rate': 0.2590104196633294, 'num_leaves': 87, 'max_depth': 4, 'min_child_samples': 47, 'subsample': 0.9927452764668567, 'colsample_bytree': 0.7576463230515209}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:30,624] Trial 34 finished with value: 0.8091573467549769 and parameters: {'n_estimators': 454, 'learning_rate': 0.24563508606882706, 'num_leaves': 131, 'max_depth': 3, 'min_child_samples': 59, 'subsample': 0.9163432665037474, 'colsample_bytree': 0.7372943484660875}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:31,333] Trial 35 finished with value: 0.8093865699306045 and parameters: {'n_estimators': 274, 'learning_rate': 0.06889697587594613, 'num_leaves': 254, 'max_depth': 5, 'min_child_samples': 73, 'subsample': 0.890392974837862, 'colsample_bytree': 0.7018024699393514}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:31,973] Trial 36 finished with value: 0.8090425366482463 and parameters: {'n_estimators': 342, 'learning_rate': 0.1807008359669742, 'num_leaves': 192, 'max_depth': 4, 'min_child_samples': 73, 'subsample': 0.7998620616716324, 'colsample_bytree': 0.6315631126001744}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:32,702] Trial 37 finished with value: 0.8034053273212983 and parameters: {'n_estimators': 542, 'learning_rate': 0.2782144116150094, 'num_leaves': 21, 'max_depth': 3, 'min_child_samples': 51, 'subsample': 0.9697424953769174, 'colsample_bytree': 0.6834965711326699}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:33,247] Trial 38 finished with value: 0.8077764491715473 and parameters: {'n_estimators': 218, 'learning_rate': 0.15142225220657898, 'num_leaves': 113, 'max_depth': 5, 'min_child_samples': 83, 'subsample': 0.704023326628279, 'colsample_bytree': 0.7797139855658418}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:38,090] Trial 39 finished with value: 0.78695459409168 and parameters: {'n_estimators': 886, 'learning_rate': 0.23158891175340995, 'num_leaves': 150, 'max_depth': 11, 'min_child_samples': 55, 'subsample': 0.9305909979911358, 'colsample_bytree': 0.8562938289101142}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:38,497] Trial 40 finished with value: 0.810537516435712 and parameters: {'n_estimators': 177, 'learning_rate': 0.20558343918707675, 'num_leaves': 173, 'max_depth': 4, 'min_child_samples': 37, 'subsample': 0.9466063395677964, 'colsample_bytree': 0.6657838319537107}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:38,856] Trial 41 finished with value: 0.8091568173711993 and parameters: {'n_estimators': 143, 'learning_rate': 0.20530557196452198, 'num_leaves': 165, 'max_depth': 4, 'min_child_samples': 34, 'subsample': 0.9561956454682892, 'colsample_bytree': 0.6613136246255324}. Best is trial 21 with value: 0.8106519295046093.\n",
      "[I 2025-07-21 14:07:39,216] Trial 42 finished with value: 0.8108823437937362 and parameters: {'n_estimators': 210, 'learning_rate': 0.24614800632715833, 'num_leaves': 132, 'max_depth': 3, 'min_child_samples': 45, 'subsample': 0.9812099522893732, 'colsample_bytree': 0.7029058552543294}. Best is trial 42 with value: 0.8108823437937362.\n",
      "[I 2025-07-21 14:07:39,591] Trial 43 finished with value: 0.8098468691250809 and parameters: {'n_estimators': 219, 'learning_rate': 0.25310169164840823, 'num_leaves': 189, 'max_depth': 3, 'min_child_samples': 38, 'subsample': 0.9470951437876589, 'colsample_bytree': 0.7041801138349124}. Best is trial 42 with value: 0.8108823437937362.\n",
      "[I 2025-07-21 14:07:39,945] Trial 44 finished with value: 0.8107681292437554 and parameters: {'n_estimators': 194, 'learning_rate': 0.26348599257455857, 'num_leaves': 185, 'max_depth': 3, 'min_child_samples': 41, 'subsample': 0.9839083688057234, 'colsample_bytree': 0.7015264405222869}. Best is trial 42 with value: 0.8108823437937362.\n",
      "[I 2025-07-21 14:07:40,294] Trial 45 finished with value: 0.8074310924297456 and parameters: {'n_estimators': 185, 'learning_rate': 0.2654897198977579, 'num_leaves': 300, 'max_depth': 3, 'min_child_samples': 43, 'subsample': 0.9879552156309663, 'colsample_bytree': 0.7343922276285405}. Best is trial 42 with value: 0.8108823437937362.\n",
      "[I 2025-07-21 14:07:40,742] Trial 46 finished with value: 0.8063964118367565 and parameters: {'n_estimators': 278, 'learning_rate': 0.2998395640679233, 'num_leaves': 181, 'max_depth': 3, 'min_child_samples': 31, 'subsample': 0.9965893764282274, 'colsample_bytree': 0.6358435533309535}. Best is trial 42 with value: 0.8108823437937362.\n",
      "[I 2025-07-21 14:07:41,099] Trial 47 finished with value: 0.8091564865063384 and parameters: {'n_estimators': 192, 'learning_rate': 0.24191119049851864, 'num_leaves': 233, 'max_depth': 3, 'min_child_samples': 55, 'subsample': 0.9242544433543888, 'colsample_bytree': 0.6983961307383432}. Best is trial 42 with value: 0.8108823437937362.\n",
      "[I 2025-07-21 14:07:41,627] Trial 48 finished with value: 0.805475945793748 and parameters: {'n_estimators': 259, 'learning_rate': 0.22547857023139212, 'num_leaves': 172, 'max_depth': 4, 'min_child_samples': 100, 'subsample': 0.9758104365225682, 'colsample_bytree': 0.7188462500722425}. Best is trial 42 with value: 0.8108823437937362.\n",
      "[I 2025-07-21 14:07:42,105] Trial 49 finished with value: 0.8028296224633417 and parameters: {'n_estimators': 134, 'learning_rate': 0.28683632803940673, 'num_leaves': 206, 'max_depth': 6, 'min_child_samples': 42, 'subsample': 0.948065826736773, 'colsample_bytree': 0.6893995834142053}. Best is trial 42 with value: 0.8108823437937362.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "sample_submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "\n",
    "test_passenger_ids = test_df['PassengerId']\n",
    "train_ids = train_df['PassengerId']\n",
    "\n",
    "y = train_df['Transported'].copy()\n",
    "train_df.drop('Transported', axis=1, inplace=True)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "all_data['GroupId'] = all_data['PassengerId'].apply(lambda x: x.split('_')[0])\n",
    "all_data['GroupSize'] = all_data.groupby('GroupId')['PassengerId'].transform('count')\n",
    "\n",
    "all_data[['Deck', 'CabinNum', 'Side']] = all_data['Cabin'].str.split('/', expand=True)\n",
    "all_data['CabinNum'] = pd.to_numeric(all_data['CabinNum'])\n",
    "\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "all_data['NoSpend'] = (all_data['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "for col in spend_cols:\n",
    "    all_data.loc[(all_data['CryoSleep'] == True) & (all_data[col].isnull()), col] = 0\n",
    "\n",
    "numeric_cols = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = all_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    median_val = all_data[col].median()\n",
    "    all_data[col] = all_data[col].fillna(median_val)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    mode_val = all_data[col].mode()[0]\n",
    "    all_data[col] = all_data[col].fillna(mode_val)\n",
    "\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "\n",
    "all_data.drop(['PassengerId', 'Name', 'Cabin', 'GroupId'], axis=1, inplace=True)\n",
    "\n",
    "bool_cols = ['VIP', 'CryoSleep']\n",
    "for col in bool_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].astype(int)\n",
    "\n",
    "categorical_cols_to_encode = all_data.select_dtypes(include=['object']).columns\n",
    "all_data = pd.get_dummies(all_data, columns=categorical_cols_to_encode, dummy_na=False)\n",
    "\n",
    "X = all_data.iloc[:len(train_ids)]\n",
    "X_test = all_data.iloc[len(train_ids):]\n",
    "y = y.astype(int)\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(random_state=42, n_estimators=200, learning_rate=0.05, num_leaves=20)\n",
    "lgbm.fit(X, y)\n",
    "predictions = lgbm.predict(X_test)\n",
    "\n",
    "submission_preds = (predictions == 1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Transported': submission_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('../submissions/baseline_submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'baseline_submission.csv' created successfully!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Ensemble Models ---\n",
      "Training LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n",
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (Our proven pipeline)\n",
    "# =============================================================================\n",
    "\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "test_passenger_ids = test_df['PassengerId']\n",
    "train_ids = train_df['PassengerId']\n",
    "y = train_df['Transported'].copy()\n",
    "\n",
    "# Drop target for concatenation\n",
    "train_df.drop('Transported', axis=1, inplace=True)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "all_data['GroupId'] = all_data['PassengerId'].apply(lambda x: x.split('_')[0])\n",
    "all_data['GroupSize'] = all_data.groupby('GroupId')['PassengerId'].transform('count')\n",
    "all_data[['Deck', 'CabinNum', 'Side']] = all_data['Cabin'].str.split('/', expand=True)\n",
    "all_data['CabinNum'] = pd.to_numeric(all_data['CabinNum'])\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "all_data['NoSpend'] = (all_data['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "# --- Imputing Missing Values ---\n",
    "for col in spend_cols:\n",
    "    all_data.loc[(all_data['CryoSleep'] == True) & (all_data[col].isnull()), col] = 0\n",
    "\n",
    "numeric_cols = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = all_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].median())\n",
    "for col in categorical_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "\n",
    "# --- Final Prep ---\n",
    "all_data.drop(['PassengerId', 'Name', 'Cabin', 'GroupId'], axis=1, inplace=True)\n",
    "bool_cols = ['VIP', 'CryoSleep']\n",
    "for col in bool_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].astype(int)\n",
    "categorical_cols_to_encode = all_data.select_dtypes(include=['object']).columns\n",
    "all_data = pd.get_dummies(all_data, columns=categorical_cols_to_encode, dummy_na=False)\n",
    "\n",
    "# Separate train and test\n",
    "X = all_data.iloc[:len(train_ids)]\n",
    "X_test = all_data.iloc[len(train_ids):]\n",
    "y = y.astype(int)\n",
    "\n",
    "# Scale data for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Model Training\n",
    "# =============================================================================\n",
    "print(\"--- Training Ensemble Models ---\")\n",
    "\n",
    "# Define models\n",
    "lgbm = lgb.LGBMClassifier(random_state=42, n_estimators=200, learning_rate=0.05, num_leaves=20)\n",
    "xgboost = xgb.XGBClassifier(random_state=42, n_estimators=200, learning_rate=0.05, max_depth=5, use_label_encoder=False, eval_metric='logloss')\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Train models\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm.fit(X, y)\n",
    "print(\"Training XGBoost...\")\n",
    "xgboost.fit(X, y)\n",
    "print(\"Training Logistic Regression...\")\n",
    "logreg.fit(X_scaled, y) # Use scaled data for logistic regression\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Blending Predictions and Creating Submission\n",
    "# =============================================================================\n",
    "print(\"\\n--- Blending Predictions ---\")\n",
    "\n",
    "# Predict probabilities for the positive class (Transported=True)\n",
    "lgbm_probs = lgbm.predict_proba(X_test)[:, 1]\n",
    "xgboost_probs = xgboost.predict_proba(X_test)[:, 1]\n",
    "logreg_probs = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Weighted average of probabilities\n",
    "final_probs = (0.4 * lgbm_probs + 0.4 * xgboost_probs + 0.2 * logreg_probs)\n",
    "\n",
    "# Convert probabilities to final predictions (True if > 0.5)\n",
    "final_preds = (final_probs > 0.5)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({'PassengerId': test_passenger_ids, 'Transported': final_preds})\n",
    "submission.to_csv('../submissions/ensemble_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'ensemble_submission.csv' created successfully!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Stacking ---\n",
      "--- Fold 1/10 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3940, number of negative: 3883\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1909\n",
      "[LightGBM] [Info] Number of data points in the train set: 7823, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503643 -> initscore=0.014573\n",
      "[LightGBM] [Info] Start training from score 0.014573\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (Our proven pipeline)\n",
    "# =============================================================================\n",
    "\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "test_passenger_ids = test_df['PassengerId']\n",
    "train_ids = train_df['PassengerId']\n",
    "y = train_df['Transported'].copy().astype(int)\n",
    "\n",
    "train_df.drop('Transported', axis=1, inplace=True)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "all_data['GroupId'] = all_data['PassengerId'].apply(lambda x: x.split('_')[0])\n",
    "all_data['GroupSize'] = all_data.groupby('GroupId')['PassengerId'].transform('count')\n",
    "all_data[['Deck', 'CabinNum', 'Side']] = all_data['Cabin'].str.split('/', expand=True)\n",
    "all_data['CabinNum'] = pd.to_numeric(all_data['CabinNum'])\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "all_data['NoSpend'] = (all_data['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "for col in spend_cols:\n",
    "    all_data.loc[(all_data['CryoSleep'] == True) & (all_data[col].isnull()), col] = 0\n",
    "\n",
    "numeric_cols = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = all_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].median())\n",
    "for col in categorical_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "\n",
    "all_data.drop(['PassengerId', 'Name', 'Cabin', 'GroupId'], axis=1, inplace=True)\n",
    "bool_cols = ['VIP', 'CryoSleep']\n",
    "for col in bool_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].astype(int)\n",
    "categorical_cols_to_encode = all_data.select_dtypes(include=['object']).columns\n",
    "all_data = pd.get_dummies(all_data, columns=categorical_cols_to_encode, dummy_na=False)\n",
    "\n",
    "X = all_data.iloc[:len(train_ids)]\n",
    "X_test = all_data.iloc[len(train_ids):]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Stacking Implementation\n",
    "# =============================================================================\n",
    "print(\"--- Starting Stacking ---\")\n",
    "\n",
    "N_SPLITS = 10\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store out-of-fold predictions\n",
    "oof_preds_lgbm = np.zeros((len(X),))\n",
    "oof_preds_xgb = np.zeros((len(X),))\n",
    "oof_preds_logreg = np.zeros((len(X),))\n",
    "\n",
    "# Create empty arrays to store test predictions\n",
    "test_preds_lgbm = np.zeros((len(X_test),))\n",
    "test_preds_xgb = np.zeros((len(X_test),))\n",
    "test_preds_logreg = np.zeros((len(X_test),))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Scaled data for logistic regression\n",
    "    X_train_scaled, X_val_scaled = X_scaled[train_idx], X_scaled[val_idx]\n",
    "\n",
    "    # --- Train and predict with LGBM ---\n",
    "    lgbm = lgb.LGBMClassifier(random_state=42, n_estimators=200, learning_rate=0.05, num_leaves=20)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    oof_preds_lgbm[val_idx] = lgbm.predict_proba(X_val)[:, 1]\n",
    "    test_preds_lgbm += lgbm.predict_proba(X_test)[:, 1] / N_SPLITS\n",
    "\n",
    "    # --- Train and predict with XGBoost ---\n",
    "    xgboost = xgb.XGBClassifier(random_state=42, n_estimators=200, learning_rate=0.05, max_depth=5, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    oof_preds_xgb[val_idx] = xgboost.predict_proba(X_val)[:, 1]\n",
    "    test_preds_xgb += xgboost.predict_proba(X_test)[:, 1] / N_SPLITS\n",
    "\n",
    "    # --- Train and predict with Logistic Regression ---\n",
    "    logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg.fit(X_train_scaled, y_train)\n",
    "    oof_preds_logreg[val_idx] = logreg.predict_proba(X_val_scaled)[:, 1]\n",
    "    test_preds_logreg += logreg.predict_proba(X_test_scaled)[:, 1] / N_SPLITS\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Train Meta-Model and Create Submission\n",
    "# =============================================================================\n",
    "print(\"\\n--- Training Meta-Model ---\")\n",
    "\n",
    "# Create the training data for the meta-model\n",
    "meta_X = np.stack([oof_preds_lgbm, oof_preds_xgb, oof_preds_logreg], axis=1)\n",
    "\n",
    "# Create the test data for the meta-model\n",
    "meta_X_test = np.stack([test_preds_lgbm, test_preds_xgb, test_preds_logreg], axis=1)\n",
    "\n",
    "# Meta-model (Logistic Regression is a good choice)\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(meta_X, y)\n",
    "\n",
    "# Make final predictions\n",
    "final_probs = meta_model.predict_proba(meta_X_test)[:, 1]\n",
    "final_preds = (final_probs > 0.5)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({'PassengerId': test_passenger_ids, 'Transported': final_preds})\n",
    "submission.to_csv('../submissions/stacking_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'stacking_submission.csv' created successfully!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 14:06:37,079] A new study created in memory with name: no-name-07c4d031-f96a-494f-9dfa-45fe36d4bdce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Hyperparameter Tuning for LightGBM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 14:06:38,518] Trial 0 finished with value: 0.8083508967430324 and parameters: {'n_estimators': 177, 'learning_rate': 0.07353394056557556, 'num_leaves': 172, 'max_depth': 8, 'min_child_samples': 21, 'subsample': 0.9043213297223234, 'colsample_bytree': 0.8202213774733798}. Best is trial 0 with value: 0.8083508967430324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.8108823437937362\n",
      "  Params: \n",
      "    n_estimators: 210\n",
      "    learning_rate: 0.24614800632715833\n",
      "    num_leaves: 132\n",
      "    max_depth: 3\n",
      "    min_child_samples: 45\n",
      "    subsample: 0.9812099522893732\n",
      "    colsample_bytree: 0.7029058552543294\n",
      "\n",
      "--- Training Final Ensemble with Tuned LGBM ---\n",
      "Training Tuned LightGBM...\n",
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (Our proven pipeline)\n",
    "# =============================================================================\n",
    "\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "test_passenger_ids = test_df['PassengerId']\n",
    "train_ids = train_df['PassengerId']\n",
    "y = train_df['Transported'].copy().astype(int)\n",
    "\n",
    "train_df.drop('Transported', axis=1, inplace=True)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "all_data['GroupId'] = all_data['PassengerId'].apply(lambda x: x.split('_')[0])\n",
    "all_data['GroupSize'] = all_data.groupby('GroupId')['PassengerId'].transform('count')\n",
    "all_data[['Deck', 'CabinNum', 'Side']] = all_data['Cabin'].str.split('/', expand=True)\n",
    "all_data['CabinNum'] = pd.to_numeric(all_data['CabinNum'])\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "all_data['NoSpend'] = (all_data['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "for col in spend_cols:\n",
    "    all_data.loc[(all_data['CryoSleep'] == True) & (all_data[col].isnull()), col] = 0\n",
    "\n",
    "numeric_cols = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = all_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].median())\n",
    "for col in categorical_cols:\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "all_data['TotalSpend'] = all_data[spend_cols].sum(axis=1)\n",
    "\n",
    "all_data.drop(['PassengerId', 'Name', 'Cabin', 'GroupId'], axis=1, inplace=True)\n",
    "bool_cols = ['VIP', 'CryoSleep']\n",
    "for col in bool_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].astype(int)\n",
    "categorical_cols_to_encode = all_data.select_dtypes(include=['object']).columns\n",
    "all_data = pd.get_dummies(all_data, columns=categorical_cols_to_encode, dummy_na=False)\n",
    "\n",
    "X = all_data.iloc[:len(train_ids)]\n",
    "X_test = all_data.iloc[len(train_ids):]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Hyperparameter Tuning with Optuna for LightGBM\n",
    "# =============================================================================\n",
    "print(\"--- Starting Hyperparameter Tuning for LightGBM ---\")\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, preds))\n",
    "        \n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50) # 50 trials is a good balance\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "best_lgbm_params = trial.params\n",
    "best_lgbm_params['random_state'] = 42\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Final Model Training and Submission\n",
    "# =============================================================================\n",
    "print(\"\\n--- Training Final Ensemble with Tuned LGBM ---\")\n",
    "\n",
    "lgbm_tuned = lgb.LGBMClassifier(**best_lgbm_params)\n",
    "xgboost = xgb.XGBClassifier(random_state=42, n_estimators=200, learning_rate=0.05, max_depth=5, use_label_encoder=False, eval_metric='logloss')\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "print(\"Training Tuned LightGBM...\")\n",
    "lgbm_tuned.fit(X, y)\n",
    "print(\"Training XGBoost...\")\n",
    "xgboost.fit(X, y)\n",
    "print(\"Training Logistic Regression...\")\n",
    "logreg.fit(X_scaled, y)\n",
    "\n",
    "lgbm_probs = lgbm_tuned.predict_proba(X_test)[:, 1]\n",
    "xgboost_probs = xgboost.predict_proba(X_test)[:, 1]\n",
    "logreg_probs = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "final_probs = (0.4 * lgbm_probs + 0.4 * xgboost_probs + 0.2 * logreg_probs)\n",
    "final_preds = (final_probs > 0.5)\n",
    "\n",
    "submission = pd.DataFrame({'PassengerId': test_passenger_ids, 'Transported': final_preds})\n",
    "submission.to_csv('../submissions/tuned_ensemble_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'tuned_ensemble_submission.csv' created successfully!\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
