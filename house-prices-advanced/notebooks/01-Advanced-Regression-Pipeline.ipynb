{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Loading and Initial Setup\n",
    "# =============================================================================\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    train_df_raw = pd.read_csv('../data/raw/train.csv')\n",
    "    test_df_raw = pd.read_csv('../data/raw/test.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure train.csv and test.csv are in 'house-prices-advanced/data/raw/'\")\n",
    "    # Exit or handle error appropriately in a real script\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(\"Initial Train shape: \", train_df_raw.shape)\n",
    "print(\"Initial Test shape: \", test_df_raw.shape)\n",
    "\n",
    "# Store IDs and target variable, then combine data\n",
    "train_ID = train_df_raw['Id']\n",
    "test_ID = test_df_raw['Id']\n",
    "\n",
    "# Keep original SalePrice for outlier detection later\n",
    "train_sale_price = train_df_raw['SalePrice'] \n",
    "y_log = np.log1p(train_sale_price)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_df = train_df_raw.drop(['Id', 'SalePrice'], axis=1)\n",
    "test_df = test_df_raw.drop('Id', axis=1)\n",
    "\n",
    "# Combine train and test data for processing\n",
    "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "\n",
    "print(\"Combined data shape: \", all_data.shape)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Data Cleaning and Feature Engineering\n",
    "# =============================================================================\n",
    "\n",
    "# --- Filling Missing Values ---\n",
    "print(\"\\nProcessing missing values...\")\n",
    "# Categorical features where NaN means 'None'\n",
    "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "\n",
    "# Numerical features where NaN means 0\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "    \n",
    "# LotFrontage: fill with median of the neighborhood\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Other features: fill with the mode\n",
    "for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "\n",
    "if all_data.isnull().sum().sum() == 0:\n",
    "    print(\"All missing values handled.\")\n",
    "else:\n",
    "    print(\"Warning: There are still missing values.\")\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "print(\"\\nPerforming feature engineering...\")\n",
    "# Convert some numerical variables into categorical\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "\n",
    "# Create new combined features\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['TotalBath'] = all_data['FullBath'] + 0.5 * all_data['HalfBath'] + all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath']\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
    "\n",
    "# Log-transform skewed numerical features\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[abs(skewed_feats) > 0.75].index\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "print(f\"Applied log transformation to {len(skewed_feats)} skewed features.\")\n",
    "\n",
    "# --- One-Hot Encoding ---\n",
    "all_data = pd.get_dummies(all_data)\n",
    "print(\"Shape after one-hot encoding: \", all_data.shape)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Outlier Removal and Final Data Split\n",
    "# =============================================================================\n",
    "print(\"\\nRemoving outliers and finalizing datasets...\")\n",
    "# Separate data back into train and test\n",
    "X = all_data[:len(train_ID)]\n",
    "X_test = all_data[len(train_ID):]\n",
    "\n",
    "# Identify and remove outliers from X and y_log\n",
    "# We use the original GrLivArea and SalePrice for this\n",
    "outlier_indices = train_df_raw[(train_df_raw['GrLivArea'] > 4000) & (train_df_raw['SalePrice'] < 300000)].index\n",
    "\n",
    "X = X.drop(outlier_indices)\n",
    "y = y_log.drop(outlier_indices)\n",
    "\n",
    "print(f\"Removed {len(outlier_indices)} outliers.\")\n",
    "print(\"Final shape of X:\", X.shape)\n",
    "print(\"Final shape of y:\", y.shape)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Model Training and Prediction\n",
    "# =============================================================================\n",
    "# Define Models (from our best submission)\n",
    "ridge = Ridge(alpha=15)\n",
    "lasso = Lasso(alpha=0.0004, max_iter=5000)\n",
    "elasticnet = ElasticNet(alpha=0.0005, l1_ratio=0.9)\n",
    "xgb = XGBRegressor(learning_rate=0.05, n_estimators=3460,\n",
    "                   max_depth=3, min_child_weight=0,\n",
    "                   gamma=0, subsample=0.7,\n",
    "                   colsample_bytree=0.7,\n",
    "                   reg_alpha=0.005,\n",
    "                   nthread=-1,\n",
    "                   scale_pos_weight=1, seed=27,\n",
    "                   random_state=42)\n",
    "\n",
    "# Train on Full Cleaned Data\n",
    "print(\"\\nTraining models on the full, cleaned dataset... (this may take a few minutes)\")\n",
    "ridge.fit(X, y)\n",
    "lasso.fit(X, y)\n",
    "elasticnet.fit(X, y)\n",
    "xgb.fit(X, y) \n",
    "print(\"All models trained.\")\n",
    "\n",
    "# Blend Predictions\n",
    "print(\"\\nMaking and blending predictions...\")\n",
    "ridge_preds = np.expm1(ridge.predict(X_test))\n",
    "lasso_preds = np.expm1(lasso.predict(X_test))\n",
    "elasticnet_preds = np.expm1(elasticnet.predict(X_test))\n",
    "xgb_preds = np.expm1(xgb.predict(X_test))\n",
    "\n",
    "# These are the weights that gave us the 36th place score\n",
    "blended_preds = (0.35 * lasso_preds + \n",
    "                 0.10 * elasticnet_preds + \n",
    "                 0.10 * ridge_preds +\n",
    "                 0.45 * xgb_preds)\n",
    "\n",
    "# Create Submission File\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': blended_preds})\n",
    "# Save to the correct submissions folder\n",
    "submission.to_csv('../submissions/submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission.csv' created successfully in 'house-prices-advanced/submissions/'!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb # Import LightGBM\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Loading and Processing (The same as before)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- Starting Data Processing ---\")\n",
    "# Load data\n",
    "train_df_raw = pd.read_csv('../data/raw/train.csv')\n",
    "test_df_raw = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "# Prep\n",
    "train_ID = train_df_raw['Id']\n",
    "test_ID = test_df_raw['Id']\n",
    "train_sale_price = train_df_raw['SalePrice'] \n",
    "y_log = np.log1p(train_sale_price)\n",
    "train_df = train_df_raw.drop(['Id', 'SalePrice'], axis=1)\n",
    "test_df = test_df_raw.drop('Id', axis=1)\n",
    "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "\n",
    "# Missing Values\n",
    "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "\n",
    "# Feature Engineering\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['TotalBath'] = all_data['FullBath'] + 0.5 * all_data['HalfBath'] + all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath']\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[abs(skewed_feats) > 0.75].index\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "# One-Hot Encoding\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "# Outlier Removal and Data Split\n",
    "X = all_data[:len(train_ID)]\n",
    "X_test = all_data[len(train_ID):]\n",
    "outlier_indices = train_df_raw[(train_df_raw['GrLivArea'] > 4000) & (train_df_raw['SalePrice'] < 300000)].index\n",
    "X = X.drop(outlier_indices)\n",
    "y = y_log.drop(outlier_indices)\n",
    "print(\"--- Data Processing Finished ---\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Model Training and Prediction with LightGBM\n",
    "# =============================================================================\n",
    "\n",
    "# Define all models, now including LightGBM\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=15),\n",
    "    'Lasso': Lasso(alpha=0.0004, max_iter=5000),\n",
    "    'ElasticNet': ElasticNet(alpha=0.0005, l1_ratio=0.9),\n",
    "    'XGBoost': XGBRegressor(learning_rate=0.05, n_estimators=3460,\n",
    "                            max_depth=3, min_child_weight=0, gamma=0, subsample=0.7,\n",
    "                            colsample_bytree=0.7, reg_alpha=0.005, nthread=-1,\n",
    "                            scale_pos_weight=1, seed=27, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(objective='regression', num_leaves=5,\n",
    "                                  learning_rate=0.05, n_estimators=720,\n",
    "                                  max_bin=55, bagging_fraction=0.8,\n",
    "                                  bagging_freq=5, feature_fraction=0.2319,\n",
    "                                  feature_fraction_seed=9, bagging_seed=9,\n",
    "                                  min_data_in_leaf=6, min_sum_hessian_in_leaf=11,\n",
    "                                  random_state=42)\n",
    "}\n",
    "\n",
    "# Train all models and store their predictions\n",
    "predictions = {}\n",
    "print(\"\\n--- Training All Models ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X, y)\n",
    "    # Predict on test data and reverse log transform\n",
    "    predictions[name] = np.expm1(model.predict(X_test))\n",
    "print(\"--- All Models Trained ---\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Blending Predictions and Creating Submission\n",
    "# =============================================================================\n",
    "\n",
    "# Blend predictions giving most weight to XGBoost and LightGBM\n",
    "blended_preds = (0.10 * predictions['Ridge'] +\n",
    "                 0.20 * predictions['Lasso'] +\n",
    "                 0.10 * predictions['ElasticNet'] +\n",
    "                 0.30 * predictions['XGBoost'] +\n",
    "                 0.30 * predictions['LightGBM'])\n",
    "\n",
    "# Create Submission File\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': blended_preds})\n",
    "submission.to_csv('../submissions/submission_lgbm.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission_lgbm.csv' created successfully!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (The same as before)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- Starting Data Processing ---\")\n",
    "# Load data\n",
    "train_df_raw = pd.read_csv('../data/raw/train.csv')\n",
    "test_df_raw = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "# Prep\n",
    "train_ID = train_df_raw['Id']\n",
    "test_ID = test_df_raw['Id']\n",
    "train_sale_price = train_df_raw['SalePrice'] \n",
    "y_log = np.log1p(train_sale_price)\n",
    "train_df = train_df_raw.drop(['Id', 'SalePrice'], axis=1)\n",
    "test_df = test_df_raw.drop('Id', axis=1)\n",
    "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "\n",
    "# Missing Values\n",
    "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "\n",
    "# Feature Engineering\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['TotalBath'] = all_data['FullBath'] + 0.5 * all_data['HalfBath'] + all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath']\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[abs(skewed_feats) > 0.75].index\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "# One-Hot Encoding\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "# Outlier Removal and Data Split\n",
    "X = all_data[:len(train_ID)]\n",
    "X_test = all_data[len(train_ID):]\n",
    "outlier_indices = train_df_raw[(train_df_raw['GrLivArea'] > 4000) & (train_df_raw['SalePrice'] < 300000)].index\n",
    "X = X.drop(outlier_indices)\n",
    "y = y_log.drop(outlier_indices)\n",
    "print(\"--- Data Processing Finished ---\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Model Training and Prediction\n",
    "# =============================================================================\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=15),\n",
    "    'Lasso': Lasso(alpha=0.0004, max_iter=5000),\n",
    "    'ElasticNet': ElasticNet(alpha=0.0005, l1_ratio=0.9),\n",
    "    'XGBoost': XGBRegressor(learning_rate=0.05, n_estimators=3460,\n",
    "                            max_depth=3, min_child_weight=0, gamma=0, subsample=0.7,\n",
    "                            colsample_bytree=0.7, reg_alpha=0.005, nthread=-1,\n",
    "                            scale_pos_weight=1, seed=27, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(objective='regression', num_leaves=5,\n",
    "                                  learning_rate=0.05, n_estimators=720,\n",
    "                                  max_bin=55, bagging_fraction=0.8,\n",
    "                                  bagging_freq=5, feature_fraction=0.2319,\n",
    "                                  feature_fraction_seed=9, bagging_seed=9,\n",
    "                                  min_data_in_leaf=6, min_sum_hessian_in_leaf=11,\n",
    "                                  random_state=42)\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "print(\"\\n--- Training All Models ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X, y)\n",
    "    predictions[name] = np.expm1(model.predict(X_test))\n",
    "print(\"--- All Models Trained ---\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Blending Predictions (Ensemble of Ensembles) and Creating Submission\n",
    "# =============================================================================\n",
    "\n",
    "# First, create a combined prediction from the two boosting models\n",
    "boosting_preds = 0.5 * predictions['XGBoost'] + 0.5 * predictions['LightGBM']\n",
    "\n",
    "# Now, blend this combined prediction with the linear models\n",
    "# We give the boosting part the most weight\n",
    "final_blended_preds = (0.70 * boosting_preds +\n",
    "                       0.15 * predictions['Lasso'] +\n",
    "                       0.10 * predictions['Ridge'] +\n",
    "                       0.05 * predictions['ElasticNet'])\n",
    "\n",
    "\n",
    "# Create Submission File\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': final_blended_preds})\n",
    "submission.to_csv('../submissions/submission_ensemble_of_ensembles.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission_ensemble_of_ensembles.csv' created successfully!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (with updated Feature Engineering)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- Starting Data Processing ---\")\n",
    "# Load data\n",
    "train_df_raw = pd.read_csv('../data/raw/train.csv')\n",
    "test_df_raw = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "# Prep\n",
    "train_ID = train_df_raw['Id']\n",
    "test_ID = test_df_raw['Id']\n",
    "train_sale_price = train_df_raw['SalePrice']\n",
    "y_log = np.log1p(train_sale_price)\n",
    "train_df = train_df_raw.drop(['Id', 'SalePrice'], axis=1)\n",
    "test_df = test_df_raw.drop('Id', axis=1)\n",
    "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "\n",
    "# Missing Values (same as before)\n",
    "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "\n",
    "# --- NEW: Ordinal Feature Encoding ---\n",
    "print(\"\\nPerforming feature engineering with ordinal encoding...\")\n",
    "# List of features that have a clear order\n",
    "ordinal_features = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "                    'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "                    'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LotShape',\n",
    "                    'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "                    'YrSold', 'MoSold']\n",
    "\n",
    "for col in ordinal_features:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[col].values)) \n",
    "    all_data[col] = lbl.transform(list(all_data[col].values))\n",
    "\n",
    "# --- Feature Engineering (same as before, but on remaining features) ---\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['TotalBath'] = all_data['FullBath'] + 0.5 * all_data['HalfBath'] + all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath']\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
    "\n",
    "# Log-transform skewed numerical features\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[abs(skewed_feats) > 0.75].index\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "# One-Hot Encoding for the *remaining* categorical features\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "# Outlier Removal and Data Split\n",
    "X = all_data[:len(train_ID)]\n",
    "X_test = all_data[len(train_ID):]\n",
    "outlier_indices = train_df_raw[(train_df_raw['GrLivArea'] > 4000) & (train_df_raw['SalePrice'] < 300000)].index\n",
    "X = X.drop(outlier_indices)\n",
    "y = y_log.drop(outlier_indices)\n",
    "print(\"--- Data Processing Finished ---\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Model Training and Prediction (same as our best attempt)\n",
    "# =============================================================================\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=15),\n",
    "    'Lasso': Lasso(alpha=0.0004, max_iter=5000),\n",
    "    'ElasticNet': ElasticNet(alpha=0.0005, l1_ratio=0.9),\n",
    "    'XGBoost': XGBRegressor(learning_rate=0.05, n_estimators=3460,\n",
    "                            max_depth=3, min_child_weight=0, gamma=0, subsample=0.7,\n",
    "                            colsample_bytree=0.7, reg_alpha=0.005, nthread=-1,\n",
    "                            scale_pos_weight=1, seed=27, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(objective='regression', num_leaves=5,\n",
    "                                  learning_rate=0.05, n_estimators=720,\n",
    "                                  max_bin=55, bagging_fraction=0.8,\n",
    "                                  bagging_freq=5, feature_fraction=0.2319,\n",
    "                                  feature_fraction_seed=9, bagging_seed=9,\n",
    "                                  min_data_in_leaf=6, min_sum_hessian_in_leaf=11,\n",
    "                                  random_state=42)\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "print(\"\\n--- Training All Models ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X, y)\n",
    "    predictions[name] = np.expm1(model.predict(X_test))\n",
    "print(\"--- All Models Trained ---\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Blending and Submission (same as our best attempt)\n",
    "# =============================================================================\n",
    "blended_preds = (0.10 * predictions['Ridge'] +\n",
    "                 0.20 * predictions['Lasso'] +\n",
    "                 0.10 * predictions['ElasticNet'] +\n",
    "                 0.30 * predictions['XGBoost'] +\n",
    "                 0.30 * predictions['LightGBM'])\n",
    "\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': blended_preds})\n",
    "submission.to_csv('../submissions/submission_label_encoding.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission_label_encoding.csv' created successfully!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (Our Best Pipeline)\n",
    "# =============================================================================\n",
    "print(\"--- Starting Data Processing ---\")\n",
    "# Load data\n",
    "train_df_raw = pd.read_csv('../data/raw/train.csv')\n",
    "test_df_raw = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "# Prep\n",
    "train_ID = train_df_raw['Id']\n",
    "test_ID = test_df_raw['Id']\n",
    "train_sale_price = train_df_raw['SalePrice']\n",
    "y_log = np.log1p(train_sale_price)\n",
    "train_df = train_df_raw.drop(['Id', 'SalePrice'], axis=1)\n",
    "test_df = test_df_raw.drop('Id', axis=1)\n",
    "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "\n",
    "# Missing Values\n",
    "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "\n",
    "# Ordinal Feature Encoding\n",
    "ordinal_features = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "                    'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "                    'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LotShape',\n",
    "                    'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "                    'YrSold', 'MoSold']\n",
    "for col in ordinal_features:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[col].values)) \n",
    "    all_data[col] = lbl.transform(list(all_data[col].values))\n",
    "\n",
    "# Other Feature Engineering\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['TotalBath'] = all_data['FullBath'] + 0.5 * all_data['HalfBath'] + all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath']\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[abs(skewed_feats) > 0.75].index\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "# One-Hot Encoding for remaining categoricals\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "# Outlier Removal and Data Split\n",
    "X = all_data[:len(train_ID)]\n",
    "X_test = all_data[len(train_ID):]\n",
    "outlier_indices = train_df_raw[(train_df_raw['GrLivArea'] > 4000) & (train_df_raw['SalePrice'] < 300000)].index\n",
    "X = X.drop(outlier_indices)\n",
    "y = y_log.drop(outlier_indices)\n",
    "print(\"--- Data Processing Finished ---\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. OOF Predictions and Weight Optimization\n",
    "# =============================================================================\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'LightGBM': lgb.LGBMRegressor(objective='regression', num_leaves=5,\n",
    "                                  learning_rate=0.05, n_estimators=720,\n",
    "                                  max_bin=55, bagging_fraction=0.8,\n",
    "                                  bagging_freq=5, feature_fraction=0.2319,\n",
    "                                  feature_fraction_seed=9, bagging_seed=9,\n",
    "                                  min_data_in_leaf=6, min_sum_hessian_in_leaf=11,\n",
    "                                  random_state=42),\n",
    "    'XGBoost': XGBRegressor(learning_rate=0.05, n_estimators=3460,\n",
    "                            max_depth=3, min_child_weight=0, gamma=0, subsample=0.7,\n",
    "                            colsample_bytree=0.7, reg_alpha=0.005, nthread=-1,\n",
    "                            scale_pos_weight=1, seed=27, random_state=42),\n",
    "    'Lasso': Lasso(alpha=0.0004, max_iter=5000)\n",
    "}\n",
    "\n",
    "# Generate OOF predictions\n",
    "print(\"\\n--- Generating OOF Predictions (this will take time) ---\")\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "oof_train = np.zeros((X.shape[0], len(models)))\n",
    "oof_test = np.zeros((X_test.shape[0], len(models)))\n",
    "X_np, y_np, X_test_np = X.values, y.values, X_test.values\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    print(f\"Training and predicting with {name}...\")\n",
    "    test_preds_for_fold = np.zeros((X_test.shape[0], kfolds.n_splits))\n",
    "    for j, (train_idx, val_idx) in enumerate(kfolds.split(X_np)):\n",
    "        model.fit(X_np[train_idx], y_np[train_idx])\n",
    "        oof_train[val_idx, i] = model.predict(X_np[val_idx])\n",
    "        test_preds_for_fold[:, j] = model.predict(X_test_np)\n",
    "    oof_test[:, i] = test_preds_for_fold.mean(axis=1)\n",
    "\n",
    "# Find optimal weights for the blend\n",
    "print(\"\\n--- Finding Optimal Blend Weights ---\")\n",
    "def rmse_func(weights, predictions, true_values):\n",
    "    final_prediction = np.dot(predictions, weights)\n",
    "    return np.sqrt(mean_squared_error(true_values, final_prediction))\n",
    "\n",
    "initial_weights = [1/3.] * len(models)\n",
    "bounds = [(0, 1)] * len(models)\n",
    "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "\n",
    "res = minimize(rmse_func, initial_weights, args=(oof_train, y), \n",
    "               method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "optimal_weights = res.x\n",
    "\n",
    "print(\"Optimal weights found:\")\n",
    "for name, weight in zip(models.keys(), optimal_weights):\n",
    "    print(f\"{name}: {weight:.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Final Submission\n",
    "# =============================================================================\n",
    "print(\"\\n--- Creating Final Submission ---\")\n",
    "final_log_preds = np.dot(oof_test, optimal_weights)\n",
    "final_preds = np.expm1(final_log_preds)\n",
    "\n",
    "submission_final = pd.DataFrame({'Id': test_ID, 'SalePrice': final_preds})\n",
    "submission_final.to_csv('../submissions/submission_optimized_weights.csv', index=False)\n",
    "\n",
    "print(\"\\nOptimized submission 'submission_optimized_weights.csv' created successfully!\")\n",
    "print(submission_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Processing ---\n",
      "--- Data Processing Finished ---\n",
      "\n",
      "--- Step 1: Initial Training and Pseudo-Label Generation ---\n",
      "Initial training for Ridge...\n",
      "Initial training for Lasso...\n",
      "Initial training for ElasticNet...\n",
      "Initial training for XGBoost...\n",
      "Initial training for LightGBM...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1552\n",
      "[LightGBM] [Info] Number of data points in the train set: 1458, number of used features: 185\n",
      "[LightGBM] [Info] Start training from score 12.024015\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "\n",
      "--- Step 2: Creating combined dataset with pseudo-labels ---\n",
      "Shape of combined training data: (2917, 227)\n",
      "Shape of combined labels: (2917,)\n",
      "\n",
      "--- Step 3: Re-training models on the combined dataset ---\n",
      "Final training for Ridge...\n",
      "Final training for Lasso...\n",
      "Final training for ElasticNet...\n",
      "Final training for XGBoost...\n",
      "Final training for LightGBM...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1621\n",
      "[LightGBM] [Info] Number of data points in the train set: 2917, number of used features: 199\n",
      "[LightGBM] [Info] Start training from score 12.017263\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "\n",
      "--- Blending final predictions ---\n",
      "\n",
      "Submission file with Pseudo-Labeling 'submission_pseudo_labeling.csv' created successfully!\n",
      "     Id      SalePrice\n",
      "0  1461  121570.244158\n",
      "1  1462  157015.018433\n",
      "2  1463  184391.504236\n",
      "3  1464  194888.530996\n",
      "4  1465  195867.857110\n",
      "--- Data Processing Finished ---\n",
      "\n",
      "--- Training All Models ---\n",
      "Training Ridge...\n",
      "Training Lasso...\n",
      "Training ElasticNet...\n",
      "Training XGBoost...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "--- All Models Trained ---\n",
      "\n",
      "Submission file 'submission_interactions.csv' created successfully!\n",
      "     Id      SalePrice\n",
      "0  1461  121423.495696\n",
      "1  1462  158722.931241\n",
      "2  1463  183103.362573\n",
      "3  1464  194778.144480\n",
      "4  1465  192689.266021\n",
      "--- Data Processing Finished ---\n",
      "\n",
      "--- Step 1: Initial Training and Pseudo-Label Generation ---\n",
      "Initial training for Ridge...\n",
      "Initial training for Lasso...\n",
      "Initial training for ElasticNet...\n",
      "Initial training for XGBoost...\n",
      "Initial training for LightGBM...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1717\n",
      "[LightGBM] [Info] Number of data points in the train set: 1458, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.024015\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "\n",
      "--- Step 2: Creating combined dataset with pseudo-labels ---\n",
      "\n",
      "--- Step 3: Re-training models on the combined dataset ---\n",
      "Final training for Ridge...\n",
      "Final training for Lasso...\n",
      "Final training for ElasticNet...\n",
      "Final training for XGBoost...\n",
      "Final training for LightGBM...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1786\n",
      "[LightGBM] [Info] Number of data points in the train set: 2917, number of used features: 202\n",
      "[LightGBM] [Info] Start training from score 12.017148\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "\n",
      "--- Blending final predictions ---\n",
      "\n",
      "Final submission 'submission_final_push.csv' created successfully!\n",
      "     Id      SalePrice\n",
      "0  1461  121884.415999\n",
      "1  1462  157419.745825\n",
      "2  1463  181733.933351\n",
      "3  1464  193901.378115\n",
      "4  1465  193865.296457\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (Our Best Pipeline) - NO CHANGES HERE\n",
    "# =============================================================================\n",
    "print(\"--- Starting Data Processing ---\")\n",
    "train_df_raw = pd.read_csv('../data/raw/train.csv')\n",
    "test_df_raw = pd.read_csv('../data/raw/test.csv')\n",
    "train_ID = train_df_raw['Id']\n",
    "test_ID = test_df_raw['Id']\n",
    "train_sale_price = train_df_raw['SalePrice']\n",
    "y_log = np.log1p(train_sale_price)\n",
    "train_df = train_df_raw.drop(['Id', 'SalePrice'], axis=1)\n",
    "test_df = test_df_raw.drop('Id', axis=1)\n",
    "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "ordinal_features = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LotShape','PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold']\n",
    "for col in ordinal_features:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[col].values)) \n",
    "    all_data[col] = lbl.transform(list(all_data[col].values))\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['TotalBath'] = all_data['FullBath'] + 0.5 * all_data['HalfBath'] + all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath']\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[abs(skewed_feats) > 0.75].index\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "all_data = pd.get_dummies(all_data)\n",
    "X = all_data[:len(train_ID)]\n",
    "X_test = all_data[len(train_ID):]\n",
    "outlier_indices = train_df_raw[(train_df_raw['GrLivArea'] > 4000) & (train_df_raw['SalePrice'] < 300000)].index\n",
    "X = X.drop(outlier_indices)\n",
    "y = y_log.drop(outlier_indices)\n",
    "print(\"--- Data Processing Finished ---\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Pseudo-Labeling Step\n",
    "# =============================================================================\n",
    "print(\"\\n--- Step 1: Initial Training and Pseudo-Label Generation ---\")\n",
    "# Define models\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=15),\n",
    "    'Lasso': Lasso(alpha=0.0004, max_iter=5000),\n",
    "    'ElasticNet': ElasticNet(alpha=0.0005, l1_ratio=0.9),\n",
    "    'XGBoost': XGBRegressor(learning_rate=0.05, n_estimators=3460, max_depth=3, min_child_weight=0, gamma=0, subsample=0.7, colsample_bytree=0.7, reg_alpha=0.005, nthread=-1, scale_pos_weight=1, seed=27, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin=55, bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf=6, min_sum_hessian_in_leaf=11, random_state=42)\n",
    "}\n",
    "\n",
    "# Train models and get initial predictions for the test set\n",
    "initial_predictions = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Initial training for {name}...\")\n",
    "    model.fit(X, y)\n",
    "    initial_predictions[name] = model.predict(X_test)\n",
    "\n",
    "# Blend the initial predictions to create the pseudo-labels\n",
    "pseudo_labels_log = (0.10 * initial_predictions['Ridge'] +\n",
    "                     0.20 * initial_predictions['Lasso'] +\n",
    "                     0.10 * initial_predictions['ElasticNet'] +\n",
    "                     0.30 * initial_predictions['XGBoost'] +\n",
    "                     0.30 * initial_predictions['LightGBM'])\n",
    "\n",
    "# --- Create the new, combined dataset ---\n",
    "print(\"\\n--- Step 2: Creating combined dataset with pseudo-labels ---\")\n",
    "X_combined = pd.concat([X, X_test]).reset_index(drop=True)\n",
    "y_combined = np.concatenate([y, pseudo_labels_log])\n",
    "\n",
    "print(\"Shape of combined training data:\", X_combined.shape)\n",
    "print(\"Shape of combined labels:\", y_combined.shape)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Final Training and Submission\n",
    "# =============================================================================\n",
    "print(\"\\n--- Step 3: Re-training models on the combined dataset ---\")\n",
    "final_predictions = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Final training for {name}...\")\n",
    "    # Re-initialize the model to be safe\n",
    "    model.fit(X_combined, y_combined)\n",
    "    final_predictions[name] = np.expm1(model.predict(X_test))\n",
    "\n",
    "# Blend the final predictions\n",
    "print(\"\\n--- Blending final predictions ---\")\n",
    "final_blended_preds = (0.10 * final_predictions['Ridge'] +\n",
    "                       0.20 * final_predictions['Lasso'] +\n",
    "                       0.10 * final_predictions['ElasticNet'] +\n",
    "                       0.30 * final_predictions['XGBoost'] +\n",
    "                       0.30 * final_predictions['LightGBM'])\n",
    "\n",
    "# Create Submission File\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': final_blended_preds})\n",
    "submission.to_csv('../submissions/submission_pseudo_labeling.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file with Pseudo-Labeling 'submission_pseudo_labeling.csv' created successfully!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Processing ---\n",
      "\n",
      "Performing feature engineering with interactions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PythonProjects\\KaggleCompetitions\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.255e+00, tolerance: 2.328e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1717\n",
      "[LightGBM] [Info] Number of data points in the train set: 1458, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.024015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (with Interaction Features)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- Starting Data Processing ---\")\n",
    "# Load data and initial prep\n",
    "train_df_raw = pd.read_csv('../data/raw/train.csv')\n",
    "test_df_raw = pd.read_csv('../data/raw/test.csv')\n",
    "train_ID = train_df_raw['Id']\n",
    "test_ID = test_df_raw['Id']\n",
    "train_sale_price = train_df_raw['SalePrice']\n",
    "y_log = np.log1p(train_sale_price)\n",
    "train_df = train_df_raw.drop(['Id', 'SalePrice'], axis=1)\n",
    "test_df = test_df_raw.drop('Id', axis=1)\n",
    "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "\n",
    "# Missing Values\n",
    "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "print(\"\\nPerforming feature engineering with interactions...\")\n",
    "# Ordinal Encoding (as in our best model)\n",
    "ordinal_features = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LotShape','PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold']\n",
    "for col in ordinal_features:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[col].values)) \n",
    "    all_data[col] = lbl.transform(list(all_data[col].values))\n",
    "\n",
    "# Combined Features\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['TotalBath'] = all_data['FullBath'] + 0.5 * all_data['HalfBath'] + all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath']\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
    "\n",
    "# --- NEW: Interaction Features ---\n",
    "# We multiply some of the most important features together\n",
    "all_data['OverallQual_x_TotalSF'] = all_data['OverallQual'] * all_data['TotalSF']\n",
    "all_data['GrLivArea_x_OverallQual'] = all_data['GrLivArea'] * all_data['OverallQual']\n",
    "# Note: YearBuilt is not label-encoded, it's a raw number, which is fine\n",
    "all_data['YearBuilt_x_OverallQual'] = all_data['YearBuilt'] * all_data['OverallQual']\n",
    "\n",
    "\n",
    "# Log-transform skewed numerical features\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[abs(skewed_feats) > 0.75].index\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "# One-Hot Encoding for remaining categoricals\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "# Outlier Removal and Data Split\n",
    "X = all_data[:len(train_ID)]\n",
    "X_test = all_data[len(train_ID):]\n",
    "outlier_indices = train_df_raw[(train_df_raw['GrLivArea'] > 4000) & (train_df_raw['SalePrice'] < 300000)].index\n",
    "X = X.drop(outlier_indices)\n",
    "y = y_log.drop(outlier_indices)\n",
    "print(\"--- Data Processing Finished ---\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Model Training and Prediction (using our best blend)\n",
    "# =============================================================================\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=15), 'Lasso': Lasso(alpha=0.0004, max_iter=5000), 'ElasticNet': ElasticNet(alpha=0.0005, l1_ratio=0.9),\n",
    "    'XGBoost': XGBRegressor(learning_rate=0.05, n_estimators=3460, max_depth=3, min_child_weight=0, gamma=0, subsample=0.7, colsample_bytree=0.7, reg_alpha=0.005, nthread=-1, scale_pos_weight=1, seed=27, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin=55, bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf=6, min_sum_hessian_in_leaf=11, random_state=42)\n",
    "}\n",
    "predictions = {}\n",
    "print(\"\\n--- Training All Models ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X, y)\n",
    "    predictions[name] = np.expm1(model.predict(X_test))\n",
    "print(\"--- All Models Trained ---\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Blending and Submission (using our best weights)\n",
    "# =============================================================================\n",
    "blended_preds = (0.10 * predictions['Ridge'] + 0.20 * predictions['Lasso'] + 0.10 * predictions['ElasticNet'] + 0.30 * predictions['XGBoost'] + 0.30 * predictions['LightGBM'])\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': blended_preds})\n",
    "submission.to_csv('../submissions/submission_interactions.csv', index=False)\n",
    "print(\"\\nSubmission file 'submission_interactions.csv' created successfully!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Processing ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') # Suppress convergence warnings for this final run\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Processing (Our absolute best pipeline with interactions)\n",
    "# =============================================================================\n",
    "print(\"--- Starting Data Processing ---\")\n",
    "train_df_raw = pd.read_csv('../data/raw/train.csv')\n",
    "test_df_raw = pd.read_csv('../data/raw/test.csv')\n",
    "train_ID = train_df_raw['Id']\n",
    "test_ID = test_df_raw['Id']\n",
    "train_sale_price = train_df_raw['SalePrice']\n",
    "y_log = np.log1p(train_sale_price)\n",
    "train_df = train_df_raw.drop(['Id', 'SalePrice'], axis=1)\n",
    "test_df = test_df_raw.drop('Id', axis=1)\n",
    "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n",
    "    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "ordinal_features = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LotShape','PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold']\n",
    "for col in ordinal_features:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[col].values)) \n",
    "    all_data[col] = lbl.transform(list(all_data[col].values))\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['TotalBath'] = all_data['FullBath'] + 0.5 * all_data['HalfBath'] + all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath']\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
    "all_data['OverallQual_x_TotalSF'] = all_data['OverallQual'] * all_data['TotalSF']\n",
    "all_data['GrLivArea_x_OverallQual'] = all_data['GrLivArea'] * all_data['OverallQual']\n",
    "all_data['YearBuilt_x_OverallQual'] = all_data['YearBuilt'] * all_data['OverallQual']\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[abs(skewed_feats) > 0.75].index\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "all_data = pd.get_dummies(all_data)\n",
    "X = all_data[:len(train_ID)]\n",
    "X_test = all_data[len(train_ID):]\n",
    "outlier_indices = train_df_raw[(train_df_raw['GrLivArea'] > 4000) & (train_df_raw['SalePrice'] < 300000)].index\n",
    "X = X.drop(outlier_indices)\n",
    "y = y_log.drop(outlier_indices)\n",
    "print(\"--- Data Processing Finished ---\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Pseudo-Labeling Step\n",
    "# =============================================================================\n",
    "print(\"\\n--- Step 1: Initial Training and Pseudo-Label Generation ---\")\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=15), 'Lasso': Lasso(alpha=0.0004, max_iter=10000), 'ElasticNet': ElasticNet(alpha=0.0005, l1_ratio=0.9, max_iter=10000),\n",
    "    'XGBoost': XGBRegressor(learning_rate=0.05, n_estimators=3460, max_depth=3, min_child_weight=0, gamma=0, subsample=0.7, colsample_bytree=0.7, reg_alpha=0.005, nthread=-1, scale_pos_weight=1, seed=27, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin=55, bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf=6, min_sum_hessian_in_leaf=11, random_state=42)\n",
    "}\n",
    "initial_predictions = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Initial training for {name}...\")\n",
    "    model.fit(X, y)\n",
    "    initial_predictions[name] = model.predict(X_test)\n",
    "\n",
    "pseudo_labels_log = (0.10 * initial_predictions['Ridge'] + 0.20 * initial_predictions['Lasso'] + 0.10 * initial_predictions['ElasticNet'] + 0.30 * initial_predictions['XGBoost'] + 0.30 * initial_predictions['LightGBM'])\n",
    "\n",
    "print(\"\\n--- Step 2: Creating combined dataset with pseudo-labels ---\")\n",
    "X_combined = pd.concat([X, X_test]).reset_index(drop=True)\n",
    "y_combined = np.concatenate([y, pseudo_labels_log])\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Final Training and Submission\n",
    "# =============================================================================\n",
    "print(\"\\n--- Step 3: Re-training models on the combined dataset ---\")\n",
    "final_predictions = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Final training for {name}...\")\n",
    "    model.fit(X_combined, y_combined)\n",
    "    final_predictions[name] = np.expm1(model.predict(X_test))\n",
    "\n",
    "print(\"\\n--- Blending final predictions ---\")\n",
    "final_blended_preds = (0.10 * final_predictions['Ridge'] + 0.20 * final_predictions['Lasso'] + 0.10 * final_predictions['ElasticNet'] + 0.30 * final_predictions['XGBoost'] + 0.30 * final_predictions['LightGBM'])\n",
    "\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': final_blended_preds})\n",
    "submission.to_csv('../submissions/submission_final_push.csv', index=False)\n",
    "print(\"\\nFinal submission 'submission_final_push.csv' created successfully!\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
